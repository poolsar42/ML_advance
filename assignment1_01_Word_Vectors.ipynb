{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Copy of assignment1_01_Word_Vectors.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poolsar42/ML_advance/blob/master/assignment1_01_Word_Vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPnQi_H7mWYa"
      },
      "source": [
        "## Assignment 01. Simple text processing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6q2LffDimWYg"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "from IPython import display"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DI_7LMO5mWYh"
      },
      "source": [
        "### Toxic or not\n",
        "Your main goal in this assignment is to classify, whether the comments are toxic or not. And practice with both classical approaches and PyTorch in the process.\n",
        "\n",
        "*Credits: This homework is inspired by YSDA NLP_course.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqKuO-FzmWYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96f5c60-0854-4dd4-cd27-0c025cca2190"
      },
      "source": [
        "# In colab run this cell\n",
        "!wget https://raw.githubusercontent.com/poolsar42/ML_advance/utils.py -nc\n",
        "!wget https://raw.githubusercontent.com/poolsar42/ML_advance/features.py -nc"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-09 14:38:58--  https://raw.githubusercontent.com/poolsar42/ML_advance/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 400 Bad Request\n",
            "2021-02-09 14:38:58 ERROR 400: Bad Request.\n",
            "\n",
            "--2021-02-09 14:38:58--  https://raw.githubusercontent.com/poolsar42/ML_advance/features.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 400 Bad Request\n",
            "2021-02-09 14:38:58 ERROR 400: Bad Request.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EDgmmIWYNN1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a69dbab-48dc-45ff-ac88-d779553df5fd"
      },
      "source": [
        "# In colab run this cell\n",
        "! wget https://raw.githubusercontent.com/poolsar42/ML_advance/blob/master/utils.py -nc\n",
        "! wget https://raw.githubusercontent.com/poolsar42/ML_advance/blob/master/features.py -nc"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘utils.py’ already there; not retrieving.\n",
            "\n",
            "File ‘features.py’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEjJfiqLmWYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c535cf8-e68a-49de-94f8-55b15ebccc06"
      },
      "source": [
        "try:\n",
        "    data = pd.read_csv('../../datasets/comments_small_dataset/comments.tsv', sep='\\t')\n",
        "except FileNotFoundError:\n",
        "    ! wget https://raw.githubusercontent.com/girafe-ai/ml-mipt/advanced_f20/datasets/comments_small_dataset/comments.tsv -nc\n",
        "    data = pd.read_csv(\"comments.tsv\", sep='\\t')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-09 13:15:35--  https://raw.githubusercontent.com/girafe-ai/ml-mipt/advanced_f20/datasets/comments_small_dataset/comments.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 353358 (345K) [text/plain]\n",
            "Saving to: ‘comments.tsv’\n",
            "\n",
            "\rcomments.tsv          0%[                    ]       0  --.-KB/s               \rcomments.tsv        100%[===================>] 345.08K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-02-09 13:15:36 (13.7 MB/s) - ‘comments.tsv’ saved [353358/353358]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMhfs9XXmWYk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "66b76732-2521-48f5-bf83-f85b774d1959"
      },
      "source": [
        "texts = data['comment_text'].values\n",
        "target = data['should_ban'].values\n",
        "data[50::200]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>should_ban</th>\n",
              "      <th>comment_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0</td>\n",
              "      <td>\"Those who're in advantageous positions are th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>1</td>\n",
              "      <td>Fartsalot56 says f**k you motherclucker!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>1</td>\n",
              "      <td>Are you a fool? \\n\\nI am sorry, but you seem t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>650</th>\n",
              "      <td>1</td>\n",
              "      <td>I AM NOT A VANDAL!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>850</th>\n",
              "      <td>0</td>\n",
              "      <td>Citing sources\\n\\nCheck out the Wikipedia:Citi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     should_ban                                       comment_text\n",
              "50            0  \"Those who're in advantageous positions are th...\n",
              "250           1          Fartsalot56 says f**k you motherclucker!!\n",
              "450           1  Are you a fool? \\n\\nI am sorry, but you seem t...\n",
              "650           1    I AM NOT A VANDAL!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
              "850           0  Citing sources\\n\\nCheck out the Wikipedia:Citi..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNUc2VaJmWYk"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "texts_train, texts_test, y_train, y_test = train_test_split(texts, target, test_size=0.5, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJqPCHP3mWYk"
      },
      "source": [
        "__Note:__ it is generally a good idea to split data into train/test before anything is done to them.\n",
        "\n",
        "It guards you against possible data leakage in the preprocessing stage. For example, should you decide to select words present in obscene tweets as features, you should only count those words over the training set. Otherwise your algoritm can cheat evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDCuOiR1mWYk"
      },
      "source": [
        "### Preprocessing and tokenization\n",
        "\n",
        "Comments contain raw text with punctuation, upper/lowercase letters and even newline symbols.\n",
        "\n",
        "To simplify all further steps, we'll split text into space-separated tokens using one of nltk tokenizers.\n",
        "\n",
        "Generally, library `nltk` [link](https://www.nltk.org) is widely used in NLP. It is not necessary in here, but mentioned to intoduce it to you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxJtH_6mmWYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e7bfd6e-183a-4990-d513-1b6252bb8c0e"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tokenizer = TweetTokenizer()\n",
        "preprocess = lambda text: ' '.join(tokenizer.tokenize(text.lower()))\n",
        "\n",
        "text = 'How to be a grown-up at work: replace \"I don\\'t want to do that\" with \"Ok, great!\".'\n",
        "print(\"before:\", text,)\n",
        "print(\"after:\", preprocess(text),)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before: How to be a grown-up at work: replace \"I don't want to do that\" with \"Ok, great!\".\n",
            "after: how to be a grown-up at work : replace \" i don't want to do that \" with \" ok , great ! \" .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqvzJc23mWYl"
      },
      "source": [
        "# task: preprocess each comment in train and test\n",
        "\n",
        "#text = [tokenizer.tokenize(text.lower()) for text in text] - from sem\n",
        "\n",
        "texts_train = [preprocess(text) for text in texts_train]\n",
        "texts_test = [preprocess(text) for text in texts_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXGLckDMmWYr"
      },
      "source": [
        "# Small check that everything is done properly\n",
        "assert texts_train[5] ==  'who cares anymore . they attack with impunity .'\n",
        "assert texts_test[89] == 'hey todds ! quick q ? why are you so gay'\n",
        "assert len(texts_test) == len(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gggWlL23_mVE",
        "outputId": "d4b37069-d029-4185-914a-828f3105d468"
      },
      "source": [
        "(list(' '.join(texts_train).split()))[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'and'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwEa09KDmWYs"
      },
      "source": [
        "### Step 1: bag of words\n",
        "\n",
        "One traditional approach to such problem is to use bag of words features:\n",
        "1. ` a vocabulary of frequent words (use train data only)\n",
        "2. for each training sample, count the number of times a word occurs in it (for each word in vocabulary).\n",
        "3. consider this count a feature for some classifier\n",
        "\n",
        "__Note:__ in practice, you can compute such features using sklearn. __Please don't do that in the current assignment, though.__\n",
        "* `from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfYOR8zfmWYs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "502d6ea3-d3db-4884-a4e3-7ba18585b4b1"
      },
      "source": [
        "from features import BoW\n",
        "\n",
        "# task: find up to k most frequent tokens in texts_train,\n",
        "# sort them by number of occurences (highest first)\n",
        "k = min(10000, len(set(' '.join(texts_train).split())))\n",
        "\n",
        "#<YOUR CODE>\n",
        "\n",
        "words = dict.fromkeys(set(' '.join(texts_train).split()), 0)\n",
        "for word in list(' '.join(texts_train).split()):\n",
        "  words[word] += 1\n",
        "words = dict(sorted(words.items(), key=lambda item: -item[1]))\n",
        "words = list(words.keys())\n",
        "self.bow = words\n",
        "\n",
        "bow = BoW(k)\n",
        "bow.fit(texts_train)\n",
        "\n",
        "#print('example features:', sorted(bow.get_vocabulary())[::100])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-7defcaa9bb61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBoW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ7YD4cgmWYs"
      },
      "source": [
        "X_train_bow = bow.transform(texts_train)\n",
        "X_test_bow = bow.transform(texts_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUOdqrxFmWYt"
      },
      "source": [
        "# Small check that everything is done properly\n",
        "bow_vocabulary = bow.get_vocabulary()\n",
        "k_max = len(set(' '.join(texts_train).split()))\n",
        "assert X_train_bow.shape == (len(texts_train), min(k, k_max))\n",
        "assert X_test_bow.shape == (len(texts_test), min(k, k_max))\n",
        "assert np.all(X_train_bow[5:10].sum(-1) == np.array([len(s.split()) for s in  texts_train[5:10]]))\n",
        "assert len(bow_vocabulary) <= min(k, k_max)\n",
        "assert X_train_bow[6, bow_vocabulary.index('.')] == texts_train[6].split().count('.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue-5eLDImWYt"
      },
      "source": [
        "Now let's do the trick with `sklearn` logistic regression implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA-Upr1rmWYt"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "bow_model = LogisticRegression().fit(X_train_bow, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KfIM5pnmWYt"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "for name, X, y, model in [\n",
        "    ('train', X_train_bow, y_train, bow_model),\n",
        "    ('test ', X_test_bow, y_test, bow_model)\n",
        "]:\n",
        "    proba = model.predict_proba(X)[:, 1]\n",
        "    auc = roc_auc_score(y, proba)\n",
        "    plt.plot(*roc_curve(y, proba)[:2], label='%s AUC=%.4f' % (name, auc))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], '--', color='black',)\n",
        "plt.legend(fontsize='large')\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frN35NycmWYt"
      },
      "source": [
        "Seems alright. Now let's create the simple logistic regression using PyTorch. Just like in the classwork."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlBHsbmFmWYu"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
        "\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAeHwJpWmWYu"
      },
      "source": [
        "from utils import plot_train_process"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhUTUZZjmWYu"
      },
      "source": [
        "model = nn.Sequential()\n",
        "\n",
        "model.add_module('l1', ### YOUR CODE HERE\n",
        "### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr1dOZR2mWYu"
      },
      "source": [
        "Remember what we discussed about loss functions! `nn.CrossEntropyLoss` combines both log-softmax and `NLLLoss`.\n",
        "\n",
        "__Be careful with it! Criterion `nn.CrossEntropyLoss` with still work with log-softmax output, but it won't allow you to converge to the optimum.__ Next comes small demonstration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9d5RXjymWYu"
      },
      "source": [
        "# loss_function = nn.NLLLoss()\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udKyOWYMmWYv"
      },
      "source": [
        "opt = ### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyDyX6c9mWYv"
      },
      "source": [
        "X_train_bow_torch = ### YOUR CODE HERE\n",
        "X_test_bow_torch = ### YOUR CODE HERE\n",
        "\n",
        "y_train_torch = ### YOUR CODE HERE\n",
        "y_test_torch = ### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW8kgHFUmWYv"
      },
      "source": [
        "Let's test that everything is fine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzYB_SuqmWYv"
      },
      "source": [
        "# example loss\n",
        "loss = loss_function(model(X_train_bow_torch[:3]), y_train_torch[:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mq2sdximWYv"
      },
      "source": [
        "assert type(loss.item()) == float"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlDrsqUHmWYv"
      },
      "source": [
        "Here comes small function to train the model. In future we will take in into separate file, but for this homework it's ok to implement it here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLOfmcCwmWYw"
      },
      "source": [
        "def train_model(\n",
        "    model,\n",
        "    opt,\n",
        "    lr_scheduler,\n",
        "    X_train_torch,\n",
        "    y_train_torch,\n",
        "    X_val_torch,\n",
        "    y_val_torch,\n",
        "    n_iterations=500,\n",
        "    batch_size=32,\n",
        "    warm_start=False,\n",
        "    show_plots=True,\n",
        "    eval_every=10\n",
        "):\n",
        "    if not warm_start:\n",
        "        for name, module in model.named_children():\n",
        "            print('resetting ', name)\n",
        "            try:\n",
        "                module.reset_parameters()\n",
        "            except AttributeError as e:\n",
        "                print('Cannot reset {} module parameters: {}'.format(name, e))\n",
        "\n",
        "    train_loss_history = []\n",
        "    train_acc_history = []\n",
        "    val_loss_history = []\n",
        "    val_acc_history = []\n",
        "\n",
        "    local_train_loss_history = []\n",
        "    local_train_acc_history = []\n",
        "    for i in range(n_iterations):\n",
        "\n",
        "        # sample 256 random observations\n",
        "        ix = np.random.randint(0, len(X_train_torch), batch_size)\n",
        "        x_batch = X_train_torch[ix]\n",
        "        y_batch = y_train_torch[ix]\n",
        "\n",
        "        # predict log-probabilities or logits\n",
        "        y_predicted = ### YOUR CODE\n",
        "\n",
        "        # compute loss, just like before\n",
        "        ### YOUR CODE\n",
        "\n",
        "\n",
        "        # compute gradients\n",
        "        ### YOUR CODE\n",
        "\n",
        "        # Adam step\n",
        "        ### YOUR CODE\n",
        "\n",
        "        # clear gradients\n",
        "        ### YOUR CODE\n",
        "\n",
        "\n",
        "        local_train_loss_history.append(loss.data.numpy())\n",
        "        local_train_acc_history.append(\n",
        "            accuracy_score(\n",
        "                y_batch.to('cpu').detach().numpy(),\n",
        "                y_predicted.to('cpu').detach().numpy().argmax(axis=1)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if i % eval_every == 0:\n",
        "            train_loss_history.append(np.mean(local_train_loss_history))\n",
        "            train_acc_history.append(np.mean(local_train_acc_history))\n",
        "            local_train_loss_history, local_train_acc_history = [], []\n",
        "\n",
        "            predictions_val = model(X_val_torch)\n",
        "            val_loss_history.append(loss_function(predictions_val, y_val_torch).to('cpu').detach().item())\n",
        "\n",
        "            acc_score_val = accuracy_score(y_val_torch.cpu().numpy(), predictions_val.to('cpu').detach().numpy().argmax(axis=1))\n",
        "            val_acc_history.append(acc_score_val)\n",
        "            lr_scheduler.step(train_loss_history[-1])\n",
        "\n",
        "            if show_plots:\n",
        "                display.clear_output(wait=True)\n",
        "                plot_train_process(train_loss_history, val_loss_history, train_acc_history, val_acc_history)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGukmNNimWYw"
      },
      "source": [
        "Let's run it on the data. Note, that here we use the `test` part of the data for validation. It's not so good idea in general, but in this task our main goal is practice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8itD3nzmWYw"
      },
      "source": [
        "train_model(model, opt, lr_scheduler, X_train_bow_torch, y_train_torch, X_test_bow_torch, y_test_torch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-8C3bfjmWYx"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "for name, X, y, model in [\n",
        "    ('train', X_train_bow_torch, y_train, model),\n",
        "    ('test ', X_test_bow_torch, y_test, model)\n",
        "]:\n",
        "    proba = model(X).detach().cpu().numpy()[:, 1]\n",
        "    auc = roc_auc_score(y, proba)\n",
        "    plt.plot(*roc_curve(y, proba)[:2], label='%s AUC=%.4f' % (name, auc))\n",
        "\n",
        "plt.plot([0, 1], [0, 1], '--', color='black',)\n",
        "plt.legend(fontsize='large')\n",
        "plt.grid()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYKxXGiTmWYx"
      },
      "source": [
        "Try to vary the number of tokens `k` and check how the model performance changes. Show it on a plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGTUUrj9mWYx"
      },
      "source": [
        "# Your beautiful code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FU0i-kpjmWYx"
      },
      "source": [
        "### Step 2: implement TF-IDF features\n",
        "\n",
        "Not all words are equally useful. One can prioritize rare words and downscale words like \"and\"/\"or\" by using __tf-idf features__. This abbreviation stands for __text frequency/inverse document frequence__ and means exactly that:\n",
        "\n",
        "$$ feature_i = { Count(word_i \\in x) \\times { log {N \\over Count(word_i \\in D) + \\alpha} }}, $$\n",
        "\n",
        "\n",
        "where x is a single text, D is your dataset (a collection of texts), N is a total number of documents and $\\alpha$ is a smoothing hyperparameter (typically 1). \n",
        "And $Count(word_i \\in D)$ is the number of documents where $word_i$ appears.\n",
        "\n",
        "It may also be a good idea to normalize each data sample after computing tf-idf features.\n",
        "\n",
        "__Your task:__ implement tf-idf features, train a model and evaluate ROC curve. Compare it with basic BagOfWords model from above.\n",
        "\n",
        "__Please don't use sklearn/nltk builtin tf-idf vectorizers in your solution :)__ You can still use 'em for debugging though."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yeJJpQ6mWYy"
      },
      "source": [
        "Blog post about implementing the TF-IDF features from scratch: https://triton.ml/blog/tf-idf-from-scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAvZOI2QmWYy"
      },
      "source": [
        "from features import TfIdf\n",
        "\n",
        "# Your beautiful code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e6S4bIWmWYy"
      },
      "source": [
        "Same stuff about moel and optimizers here (or just omit it, if you are using the same model as before)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ayn5wwwkmWYy"
      },
      "source": [
        "### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1QVlCbJmWYy"
      },
      "source": [
        "X_train_tfidf_torch = ### YOUR CODE HERE\n",
        "X_test_tfidf_torch = ### YOUR CODE HERE\n",
        "\n",
        "y_train_torch = ### YOUR CODE HERE\n",
        "y_test_torch = ### YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fciuDD5TmWYy"
      },
      "source": [
        "Fit your model to the data. No not hesitate to vary number of iterations, learning rate and so on.\n",
        "\n",
        "_Note: due to very small dataset, increasing the complexity of the network might not be the best idea._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89mu2-Y6mWYy"
      },
      "source": [
        "### Step 3: Comparing it with Naive Bayes\n",
        "\n",
        "Naive Bayes classifier is a good choice for such small problems. Try to tune it for both BOW and TF-iDF features. Compare the results with Logistic Regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoCmd8K1mWYy"
      },
      "source": [
        "# Your beautiful code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY_guuFvmWYy"
      },
      "source": [
        "Shape some thoughts on the results you aquired. Which model has show the best performance? Did changing the learning rate/lr scheduler help?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN0bM9oimWYz"
      },
      "source": [
        "_Your beautiful thoughts here_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88PBlCovmWYz"
      },
      "source": [
        "### Step 4: Using the external knowledge.\n",
        "\n",
        "Use the `gensim` word2vec pretrained model to translate words into vectors. Use several models with this new encoding technique. Compare the results, share your thoughts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVLcEbu9mWYz"
      },
      "source": [
        "# Your beautiful code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}